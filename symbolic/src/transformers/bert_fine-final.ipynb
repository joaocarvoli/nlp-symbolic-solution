{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53b17244-159c-4671-8c52-81bf65185449",
   "metadata": {},
   "source": [
    "## Installs and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cd9665b-5775-425c-b645-37b17d94c3ce",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: kagglehub in ./.local/lib/python3.12/site-packages (0.3.12)\n",
      "Requirement already satisfied: packaging in /opt/tljh/user/lib/python3.12/site-packages (from kagglehub) (24.1)\n",
      "Requirement already satisfied: pyyaml in /opt/tljh/user/lib/python3.12/site-packages (from kagglehub) (6.0.1)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.12/site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/tljh/user/lib/python3.12/site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/tljh/user/lib/python3.12/site-packages (from requests->kagglehub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/tljh/user/lib/python3.12/site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/tljh/user/lib/python3.12/site-packages (from requests->kagglehub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/tljh/user/lib/python3.12/site-packages (from requests->kagglehub) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba2167ae-cb01-430a-a600-d791df9f0b2e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.12/site-packages (4.52.4)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./.local/lib/python3.12/site-packages (from transformers) (0.32.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/tljh/user/lib/python3.12/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/tljh/user/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/tljh/user/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.local/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.local/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/tljh/user/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/tljh/user/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/tljh/user/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/tljh/user/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/tljh/user/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/tljh/user/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4469f5d-9b1e-42d3-867f-c1d97e79771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f44be0f-a7e0-4b45-8487-4e8e6387f010",
   "metadata": {},
   "source": [
    "## Configuration and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a651ec67-0538-4fe5-b288-62e2f710cd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"neuralmind/bert-base-portuguese-cased\"  # BERTimbau\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(int(self.labels[idx]))\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e23a9d-a8fb-4cfe-96ba-8e7d270763cd",
   "metadata": {},
   "source": [
    "## Loading, balancing and joining Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d254ce7-6689-4fae-84bc-768c9936bb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5514/965057704.py:5: DeprecationWarning: load_dataset is deprecated and will be removed in a future version.\n",
      "  df_kaggle_unbalanced = kagglehub.load_dataset(KaggleDatasetAdapter.PANDAS,\n"
     ]
    }
   ],
   "source": [
    "### KAGGLE DATASET\n",
    "\n",
    "file_path = \"utlc_movies.csv\"\n",
    "\n",
    "df_kaggle_unbalanced = kagglehub.load_dataset(KaggleDatasetAdapter.PANDAS,\n",
    "  \"fredericods/ptbr-sentiment-analysis-datasets\", file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b69c7912-58b7-4c8f-8b71-e06ec1c4c1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### HUGGINGFACE DATASET\n",
    "\n",
    "df_HF = pd.read_parquet(\"hf://datasets/AiresPucrs/sentiment-analysis-pt/data/train-00000-of-00001.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2186f11-92df-43f8-bd0a-031c35873395",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kaggle_unbalanced = df_kaggle_unbalanced[['review_text', 'polarity']]\n",
    "df_kaggle_unbalanced.dropna(inplace=True)\n",
    "df_kaggle_unbalanced.reset_index(inplace=True)\n",
    "df_kaggle_unbalanced = df_kaggle_unbalanced[['review_text', 'polarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76ee545c-1c27-4fa1-bcc0-63904437b582",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BALANCING THE KAGGLE DATASET\n",
    "\n",
    "df_zero = df_kaggle_unbalanced[df_kaggle_unbalanced[\"polarity\"] == 0.0]\n",
    "df_one  = df_kaggle_unbalanced[df_kaggle_unbalanced[\"polarity\"] == 1.0]\n",
    "\n",
    "n0 = len(df_zero)  # size of minority class\n",
    "n1 = len(df_one)   # size of majority class\n",
    "\n",
    "# Randomly downsample the majority class to match n0\n",
    "df_one_downsampled = df_one.sample(n=n0, replace=False)#random_state=42\n",
    "\n",
    "df_balanced = pd.concat([df_zero, df_one_downsampled], axis=0)\n",
    "\n",
    "# Shuffle the combined DataFrame to mix classes\n",
    "df_balanced = df_balanced.sample(frac=1.0).reset_index(drop=True)#random_state=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9bd6180-29cf-4d9f-a0aa-027110fd5566",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HF.rename(columns = {'text':\"review_text\", \"label\":\"polarity\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f57b252b-9b13-4f62-8d22-f90278c036f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced = df_balanced.astype({\"polarity\":int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d91bffd1-aa33-4afc-9227-a00e13d7c727",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_balanced, df_HF], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7efed44e-02e9-4c0b-a8ec-9cd48dfff41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1.0)#, random_state=42)\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93ffd66d-140f-481d-867c-8f33fca19cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train, df_val = train_test_split(\n",
    "#     df,\n",
    "#     test_size=0.1,\n",
    "#     stratify=df['polarity'],\n",
    "#     random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fb2975-df5d-4e93-81f3-87a5d65c1d0b",
   "metadata": {},
   "source": [
    "## Fine tune function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90326ad1-1797-4e4c-ba70-a1068cb50e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(\n",
    "    df: pd.DataFrame,\n",
    "    output_dir: str,\n",
    "    epochs: int = 3,\n",
    "    batch_size: int = 32,\n",
    "    learning_rate: float = 3e-5,\n",
    "    eval_strategy: str = \"steps\",\n",
    "    eval_steps: int = 1500,\n",
    "    save_strategy: str = \"steps\",\n",
    "    freeze_base: bool = False,\n",
    "    ewc_lambda: float = 0.0,\n",
    "    test_size=0.1,\n",
    "    model_name: str = MODEL_NAME):\n",
    "    \n",
    "    # Split\n",
    "    df_train, df_val = train_test_split(\n",
    "        df, test_size=test_size, stratify=df['polarity'], random_state=SEED\n",
    "    )\n",
    "\n",
    "    # Load tokenizer & model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=2\n",
    "    )\n",
    "\n",
    "    # Optional: freeze base layers\n",
    "    if freeze_base:\n",
    "        for name, param in model.named_parameters():\n",
    "            if not name.startswith('classifier'):\n",
    "                param.requires_grad = False\n",
    "\n",
    "    # Datasets & DataCollator\n",
    "    train_dataset = ReviewDataset(\n",
    "        df_train['review_text'].tolist(),\n",
    "        df_train['polarity'].tolist(),\n",
    "        tokenizer\n",
    "    )\n",
    "    val_dataset = ReviewDataset(\n",
    "        df_val['review_text'].tolist(),\n",
    "        df_val['polarity'].tolist(),\n",
    "        tokenizer\n",
    "    )\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        eval_strategy=eval_strategy,\n",
    "        eval_steps=eval_steps,\n",
    "        save_strategy=save_strategy,\n",
    "        save_steps=eval_steps,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        save_total_limit=2,\n",
    "        logging_steps=100\n",
    "    )\n",
    "\n",
    "    # Optional: Elastic Weight Consolidation hook\n",
    "    compute_ewc = None\n",
    "    if ewc_lambda > 0.0:\n",
    "        # Simple L2 towards initial weights\n",
    "        init_state = {n: p.clone().detach() for n, p in model.named_parameters()}\n",
    "        def compute_ewc_loss():\n",
    "            loss = 0.0\n",
    "            for name, param in model.named_parameters():\n",
    "                # move init_state[name] to whatever device 'param' is on:\n",
    "                ref = init_state[name].to(param.device)\n",
    "                loss += ((param - ref) ** 2).sum()\n",
    "            return ewc_lambda * loss\n",
    "\n",
    "    # Custom Trainer to add EWC loss\n",
    "    class EWCTrainer(Trainer):\n",
    "        def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            if compute_ewc is not None:\n",
    "                loss = loss + compute_ewc()\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    trainer_cls = EWCTrainer if compute_ewc else Trainer\n",
    "    trainer = trainer_cls(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Train & save\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Model fine-tuned and saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8364c26e-e456-43e7-9c1f-8b1bb2066b2e",
   "metadata": {},
   "source": [
    "## Inference helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fab316e-21ab-48be-89fe-bac294b333e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_path: str):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def predict_sentiment(texts: list[str], model, tokenizer, device: str = 'cpu') -> list[int]:\n",
    "    model.to(device)\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=enc['input_ids'].to(device),\n",
    "            attention_mask=enc['attention_mask'].to(device)\n",
    "        )\n",
    "    preds = torch.argmax(outputs.logits, dim=1)\n",
    "    return preds.cpu().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6c07c4-ccc9-49f2-9d76-fc18df1d8ec1",
   "metadata": {},
   "source": [
    "## Domain-specific Fine Tune dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a68b0f8-a3b1-4ad4-8998-4e699a7653b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_link = 'https://raw.githubusercontent.com/joaocarvoli/nlp-symbolic-solution/refs/heads/main/data/all_comments.csv'\n",
    "df_movies = pd.read_csv(comments_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18b8e846-4e1f-4030-87d0-d2f45bda9533",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies['rating_label'] = (df_movies['numeric_rating'] >= 2.5).astype(int) ### mudei de 3.0 para 2.5 para ver como ficariam os resultados\n",
    "df_movies = df_movies[[\"comment\", \"rating_label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d49d8d25-6ea5-4fbb-ac7e-2af45e1c7260",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies.rename(columns={\"comment\":\"review_text\", \"rating_label\":\"polarity\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81509329-bac0-4c09-b346-06579322eec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies_FT, df_movies_test = train_test_split(\n",
    "    df_movies, test_size=2704/7704, stratify=df_movies['polarity'], random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9335ca0c-202f-4ef5-bf15-1ce4a664fc91",
   "metadata": {},
   "source": [
    "## Fine Tune and domain-specific Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16316b7c-cc60-47d1-85a0-ab3ccd415d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_5514/1309294802.py:84: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = trainer_cls(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18582' max='50640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18582/50640 11:00:21 < 18:59:23, 0.47 it/s, Epoch 1.83/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.283200</td>\n",
       "      <td>0.301549</td>\n",
       "      <td>0.873566</td>\n",
       "      <td>0.840123</td>\n",
       "      <td>0.919018</td>\n",
       "      <td>0.877801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.285200</td>\n",
       "      <td>0.258185</td>\n",
       "      <td>0.890783</td>\n",
       "      <td>0.894968</td>\n",
       "      <td>0.882545</td>\n",
       "      <td>0.888713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.273000</td>\n",
       "      <td>0.256933</td>\n",
       "      <td>0.895088</td>\n",
       "      <td>0.897414</td>\n",
       "      <td>0.889345</td>\n",
       "      <td>0.893361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.251200</td>\n",
       "      <td>0.256642</td>\n",
       "      <td>0.896976</td>\n",
       "      <td>0.876215</td>\n",
       "      <td>0.921715</td>\n",
       "      <td>0.898390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.236700</td>\n",
       "      <td>0.243494</td>\n",
       "      <td>0.900169</td>\n",
       "      <td>0.886157</td>\n",
       "      <td>0.915590</td>\n",
       "      <td>0.900633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.237000</td>\n",
       "      <td>0.251065</td>\n",
       "      <td>0.891728</td>\n",
       "      <td>0.851363</td>\n",
       "      <td>0.946049</td>\n",
       "      <td>0.896212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.185300</td>\n",
       "      <td>0.273207</td>\n",
       "      <td>0.900530</td>\n",
       "      <td>0.878865</td>\n",
       "      <td>0.926380</td>\n",
       "      <td>0.901997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.185200</td>\n",
       "      <td>0.262046</td>\n",
       "      <td>0.902252</td>\n",
       "      <td>0.887838</td>\n",
       "      <td>0.918175</td>\n",
       "      <td>0.902752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.191500</td>\n",
       "      <td>0.258647</td>\n",
       "      <td>0.902224</td>\n",
       "      <td>0.900320</td>\n",
       "      <td>0.901989</td>\n",
       "      <td>0.901154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.203000</td>\n",
       "      <td>0.252052</td>\n",
       "      <td>0.903002</td>\n",
       "      <td>0.894467</td>\n",
       "      <td>0.911206</td>\n",
       "      <td>0.902759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.201800</td>\n",
       "      <td>0.248050</td>\n",
       "      <td>0.902696</td>\n",
       "      <td>0.897519</td>\n",
       "      <td>0.906598</td>\n",
       "      <td>0.902035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.192600</td>\n",
       "      <td>0.243523</td>\n",
       "      <td>0.904501</td>\n",
       "      <td>0.887282</td>\n",
       "      <td>0.924132</td>\n",
       "      <td>0.905332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1) Initial fine-tune\n",
    "fine_tune(\n",
    "    df,\n",
    "    output_dir='./bertimbau_finetuned',\n",
    "    epochs=5,\n",
    "    learning_rate=3e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8067428-66b7-41f0-9bfd-e3bbe7830c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_194300/1962164285.py:87: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = trainer_cls(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 11:19, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.756500</td>\n",
       "      <td>0.738955</td>\n",
       "      <td>0.762204</td>\n",
       "      <td>0.856439</td>\n",
       "      <td>0.734903</td>\n",
       "      <td>0.791030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.604900</td>\n",
       "      <td>0.534155</td>\n",
       "      <td>0.764053</td>\n",
       "      <td>0.849108</td>\n",
       "      <td>0.747585</td>\n",
       "      <td>0.795119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.532400</td>\n",
       "      <td>0.500080</td>\n",
       "      <td>0.768491</td>\n",
       "      <td>0.835724</td>\n",
       "      <td>0.774155</td>\n",
       "      <td>0.803762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.559100</td>\n",
       "      <td>0.493999</td>\n",
       "      <td>0.770710</td>\n",
       "      <td>0.828680</td>\n",
       "      <td>0.788647</td>\n",
       "      <td>0.808168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.521900</td>\n",
       "      <td>0.493096</td>\n",
       "      <td>0.771450</td>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.791063</td>\n",
       "      <td>0.809141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fine-tuned and saved to ./bertimbau_adapted_5\n"
     ]
    }
   ],
   "source": [
    "# 2) Domain adaptation on new data\n",
    "fine_tune(\n",
    "    df_movies,\n",
    "    model_name='./bertimbau_finetuned/checkpoint-18000',\n",
    "    output_dir='./bertimbau_adapted_5',\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    epochs=5,\n",
    "    learning_rate=1e-5,\n",
    "    batch_size = 8,\n",
    "    #eval_steps = 500,\n",
    "    freeze_base=True,       # optional: freeze base BERT layers\n",
    "    ewc_lambda=0.1,         # optional: small EWC regularization\n",
    "    test_size=2704/7704      \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5e15be-7a70-44af-83f8-eedefce7ad3b",
   "metadata": {},
   "source": [
    "### Inference test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8f56f98-3dbf-4fc8-a64c-2ed7f5dfd12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0]\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model_and_tokenizer('./bertimbau_adapted_3')\n",
    "sample_texts = [\"Ótimo filme!\", \"Não gostei do roteiro.\"]\n",
    "print(predict_sentiment(sample_texts, model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3bd9e8-40a9-4965-bf04-1d8711111233",
   "metadata": {},
   "source": [
    "## Final evaluation of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2033d9e-b8dc-4a65-9232-0a3b4f9f01d0",
   "metadata": {},
   "source": [
    "### With only the first fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ac80991-8820-4310-8515-23d490254006",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "labels = []\n",
    "\n",
    "for i in range(len(df_movies['review_text'])):\n",
    "    texts +=[df_movies['review_text'][i]]\n",
    "    labels +=[df_movies['polarity'][i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df68d4eb-2147-478f-bdc5-53011b00838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = predict_sentiment(texts, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29119a60-77a1-4459-82bb-27aaf5cac38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score)\n",
    "\n",
    "\n",
    "def compute_classification_metrics(labels, predictions, probabilities=None):\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'precision': precision_score(labels, predictions, zero_division=0),\n",
    "        'recall': recall_score(labels, predictions, zero_division=0),\n",
    "        'f1_score': f1_score(labels, predictions, zero_division=0)\n",
    "    }\n",
    "\n",
    "    # Compute AUC only if probabilities are provided\n",
    "    if probabilities is not None:\n",
    "        metrics['auc_score'] = roc_auc_score(labels, probabilities)\n",
    "    else:\n",
    "        metrics['auc_score'] = None  # Could raise a warning or log here\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6690a01b-7713-4439-88f1-e6131e7329c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = compute_classification_metrics(labels, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209cfc9a-2c4c-4708-9b0d-c0e2d7af276d",
   "metadata": {},
   "source": [
    "### With the domain specific fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "108315e9-c744-4d21-9c28-64785d20e43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74fcb868-91ff-49dc-bf44-7515de8ed266",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_DS = []\n",
    "labels_DS = []\n",
    "\n",
    "for i in range(len(df_movies_test['review_text'])):\n",
    "    texts_DS +=[df_movies_test['review_text'][i]]\n",
    "    labels_DS +=[df_movies_test['polarity'][i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d751ae5e-e94e-4a5c-87f3-fb157764b730",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = predict_sentiment(texts_DS, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bc5860a-67bb-4982-8839-31c31cc5677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adap, tokenizer_adap = load_model_and_tokenizer('./bertimbau_adapted_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b33dd46-040f-4aaa-8939-35a7b910edcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_DS_FT = predict_sentiment(texts_DS, model_adap, tokenizer_adap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d6da325-fa46-4508-bf40-0ce3798e9f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_DS_FT = compute_classification_metrics(labels_DS, results_DS_FT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51f98b9c-cccb-4d26-a3d6-27d10fe6e464",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = compute_classification_metrics(labels_DS, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47b31627-65d1-4d0d-90be-605abaea70a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7548076923076923,\n",
       " 'precision': 0.8584615384615385,\n",
       " 'recall': 0.763129102844639,\n",
       " 'f1_score': 0.8079930495221547,\n",
       " 'auc_score': None}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_DS_FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "07c5c89c-403a-4753-beaa-29380e71e1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7267011834319527,\n",
       " 'precision': 0.8783877692842251,\n",
       " 'recall': 0.6914660831509847,\n",
       " 'f1_score': 0.7737985919804101,\n",
       " 'auc_score': None}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
